{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7716306,"sourceType":"datasetVersion","datasetId":4506506},{"sourceId":7724019,"sourceType":"datasetVersion","datasetId":4512288},{"sourceId":7724238,"sourceType":"datasetVersion","datasetId":4512433},{"sourceId":7724252,"sourceType":"datasetVersion","datasetId":4512444},{"sourceId":7770545,"sourceType":"datasetVersion","datasetId":4545837},{"sourceId":7862421,"sourceType":"datasetVersion","datasetId":4612378}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Welcome to the CCAC Green Light District Notebook submission. Point to note: Please convert data into format provided in the train data. For details on how we created the features you can email: sahoo14@purdue.edu or vponduri@purdue.edu","metadata":{}},{"cell_type":"markdown","source":"# DATA PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":"## Import the data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Attempt to read the CSV files with a specified encoding if UTF-8 fails\ntry:\n    train_df = pd.read_csv('DIWBB_Training.csv', encoding='utf-8')\nexcept UnicodeDecodeError:\n    train_df = pd.read_csv('DIWBB_Training.csv', encoding='windows-1252')  # or 'iso-8859-1' as an alternative\n\ntry:\n    test_df = pd.read_csv('DIWBB_Test.csv', encoding='utf-8')\nexcept UnicodeDecodeError:\n    test_df = pd.read_csv('DIWBB_Test.csv', encoding='windows-1252')  # or 'iso-8859-1' as an alternative\n\n# Proceed with your data processing here\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:21:53.218532Z","iopub.execute_input":"2024-03-16T20:21:53.219049Z","iopub.status.idle":"2024-03-16T20:21:58.250017Z","shell.execute_reply.started":"2024-03-16T20:21:53.219013Z","shell.execute_reply":"2024-03-16T20:21:58.248530Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Removing some features that did not help us in the final code\n\n# Drop columns from train_df\ntrain_df = train_df.drop(['state1', 'state2', 'state3', 'state4'], axis=1)\n\n# Drop columns from test_df\ntest_df = test_df.drop(['state1', 'state2', 'state3', 'state4'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:21:58.252471Z","iopub.execute_input":"2024-03-16T20:21:58.252952Z","iopub.status.idle":"2024-03-16T20:21:58.443218Z","shell.execute_reply.started":"2024-03-16T20:21:58.252913Z","shell.execute_reply":"2024-03-16T20:21:58.441753Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Converting the teams to one hot encoding.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df_train and df_test are your DataFrames\n# You would replace the sample data with your actual DataFrames\n\n# Function to one-hot encode the teams across specified columns in a DataFrame\ndef efficient_one_hot(df):\n    team_columns = ['team1', 'team2', 'team3', 'team4']\n    # Dynamically identify all unique team names from the team columns\n    unique_teams = pd.unique(df[team_columns].values.ravel('K'))\n    \n    # Create a new DataFrame to hold the one-hot encoded values\n    encoded_df = pd.DataFrame(index=df.index)\n    \n    # One-hot encode each unique team\n    for team in unique_teams:\n        # Check for the presence of the team in any of the specified columns and encode accordingly\n        encoded_df[team] = df[team_columns].isin([team]).any(axis=1).astype(int)\n    \n    return encoded_df\n\n# Example usage with a DataFrame (replace 'df_train' with your actual DataFrame)\ndf_train_encoded = efficient_one_hot(train_df)\ndf_test_encoded = efficient_one_hot(test_df)\n\n# Note: Ensure df_train and df_test are loaded or defined before applying this function.\n\n# This code handles the entire dataset, ensuring all unique teams found in 'team1', 'team2', 'team3', and 'team4' \n# across the DataFrame are included in the one-hot encoding. The result is a DataFrame where each team has its own column,\n# indicating presence (1) or absence (0) in any of the team columns for each row.\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:21:58.444830Z","iopub.execute_input":"2024-03-16T20:21:58.445280Z","iopub.status.idle":"2024-03-16T20:22:03.335631Z","shell.execute_reply.started":"2024-03-16T20:21:58.445248Z","shell.execute_reply":"2024-03-16T20:22:03.334291Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Converting 'Unknown' and 'unknown' into nulls to impute later","metadata":{}},{"cell_type":"code","source":"# Assuming df_train_encoded and df_train are already defined\n\n# Step 1: Drop the \"unknown\" column from df_train_encoded if it exists\nif 'unknown' in df_train_encoded.columns:\n    df_train_encoded.drop('unknown', axis=1, inplace=True)\nif 'Unknown' in df_train_encoded.columns:\n    df_train_encoded.drop('Unknown', axis=1, inplace=True)\n\n    \nif 'unknown' in df_test_encoded.columns:\n    df_test_encoded.drop('unknown', axis=1, inplace=True)\n    \nif 'Unknown' in df_test_encoded.columns:\n    df_test_encoded.drop('Unknown', axis=1, inplace=True)\n# Step 2: Append (or concatenate) the modified df_train_encoded to df_train\n# Make sure to align them by the index if they share the same row order\ntrain_df = pd.concat([train_df, df_train_encoded], axis=1)\ntest_df = pd.concat([test_df, df_test_encoded], axis=1)\n\n# Now, df_train_final contains the original columns from df_train plus the one-hot encoded columns from df_train_encoded, excluding the \"unknown\" column.\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:22:03.338654Z","iopub.execute_input":"2024-03-16T20:22:03.339856Z","iopub.status.idle":"2024-03-16T20:22:04.026051Z","shell.execute_reply.started":"2024-03-16T20:22:03.339795Z","shell.execute_reply":"2024-03-16T20:22:04.024675Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Additional feature engineering","metadata":{}},{"cell_type":"code","source":"\ndef feature_engineering(data):\n    # Assuming the data has already been preprocessed as per previous steps\n\n    # Feature 2: Duration between the first action and the last purchase\n    if 'DaysSinceCustomerFirstWBBActionDate' in data.columns and 'DaysSinceCustomerLastWBBPurchaseDate' in data.columns:\n        data['DurationBetweenFirstActionAndLastPurchase'] = data['DaysSinceCustomerFirstWBBActionDate'] - data['DaysSinceCustomerLastWBBPurchaseDate']\n    \n    # Feature 3: Total number of actions taken by the customer (assuming we have such data)\n    # This is a placeholder for demonstration; in practice, you would calculate based on available data\n    if 'TotalActionsByCustomer' in data.columns:\n        data['TotalActionsByCustomer'] = data['TotalActionsByCustomer']  # Assuming this column is calculated elsewhere\n\n    # Feature 4: Whether the event is in the customer's state\n    if 'CustomerState' in data.columns and 'FacilityState' in data.columns:\n        data['IsLocalEvent'] = (data['CustomerState'] == data['FacilityState']).astype('int64')\n    \n    return data\n\n# Apply feature engineering to the preprocessed data\nengineered_data = feature_engineering(train_df)\ntrain_df = engineered_data\nengineered_data_test = feature_engineering(test_df)\ntest_df =engineered_data_test \n\n# Show the first few rows of the data with new features\nengineered_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:22:04.027907Z","iopub.execute_input":"2024-03-16T20:22:04.029146Z","iopub.status.idle":"2024-03-16T20:22:04.128844Z","shell.execute_reply.started":"2024-03-16T20:22:04.029103Z","shell.execute_reply":"2024-03-16T20:22:04.127532Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   RecordID  ChampionshipYear  CustomerID CustomerCity CustomerState  \\\n0    578923              2022          15      Unknown       Unknown   \n1   1442480              2023          15      Unknown       Unknown   \n2     28140              2022          24     Pasadena            CA   \n3    851863              2023          24     Pasadena            CA   \n4    247590              2022          47      Unknown       Unknown   \n\n  CustomerZipCode CustomerInstitutionAffinity IsCustomerInNCAAMembership  \\\n0         Unknown                     Unknown                         No   \n1         Unknown                     Unknown                         No   \n2           91104                     Unknown                         No   \n3           91104                     Unknown                         No   \n4         Unknown                     Unknown                         No   \n\n  HasCustomerClickedOrOpenedEmailsSixMonthsPrior CustomerFirstWBBActionDate  \\\n0                                        Unknown                  3/26/2018   \n1                                        Unknown                  3/26/2018   \n2                                        Unknown                 10/28/2013   \n3                                        Unknown                 10/28/2013   \n4                                        Unknown                 10/28/2013   \n\n   ... IUPUI Marquette Iona Gardner-Webb American Buffalo Mercer Holy cross  \\\n0  ...     0         0    0            0        0       0      0          0   \n1  ...     0         0    0            0        0       0      0          0   \n2  ...     0         0    0            0        0       0      0          0   \n3  ...     0         0    0            0        0       0      0          0   \n4  ...     0         0    0            0        0       0      0          0   \n\n  Vermont IsLocalEvent  \n0       0            1  \n1       0            1  \n2       0            0  \n3       0            0  \n4       0            1  \n\n[5 rows x 147 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RecordID</th>\n      <th>ChampionshipYear</th>\n      <th>CustomerID</th>\n      <th>CustomerCity</th>\n      <th>CustomerState</th>\n      <th>CustomerZipCode</th>\n      <th>CustomerInstitutionAffinity</th>\n      <th>IsCustomerInNCAAMembership</th>\n      <th>HasCustomerClickedOrOpenedEmailsSixMonthsPrior</th>\n      <th>CustomerFirstWBBActionDate</th>\n      <th>...</th>\n      <th>IUPUI</th>\n      <th>Marquette</th>\n      <th>Iona</th>\n      <th>Gardner-Webb</th>\n      <th>American</th>\n      <th>Buffalo</th>\n      <th>Mercer</th>\n      <th>Holy cross</th>\n      <th>Vermont</th>\n      <th>IsLocalEvent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>578923</td>\n      <td>2022</td>\n      <td>15</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>No</td>\n      <td>Unknown</td>\n      <td>3/26/2018</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1442480</td>\n      <td>2023</td>\n      <td>15</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>No</td>\n      <td>Unknown</td>\n      <td>3/26/2018</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28140</td>\n      <td>2022</td>\n      <td>24</td>\n      <td>Pasadena</td>\n      <td>CA</td>\n      <td>91104</td>\n      <td>Unknown</td>\n      <td>No</td>\n      <td>Unknown</td>\n      <td>10/28/2013</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>851863</td>\n      <td>2023</td>\n      <td>24</td>\n      <td>Pasadena</td>\n      <td>CA</td>\n      <td>91104</td>\n      <td>Unknown</td>\n      <td>No</td>\n      <td>Unknown</td>\n      <td>10/28/2013</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>247590</td>\n      <td>2022</td>\n      <td>47</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>No</td>\n      <td>Unknown</td>\n      <td>10/28/2013</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 147 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Imputing medians.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming engineered_data is your DataFrame\nengineered_data = train_df\nengineered_data_test = test_df\n# List of columns to convert to int64\ncolumns_to_convert = ['Distance', 'Age', 'Begin_day']\n\nfor column in columns_to_convert:\n    # Replace 'Unknown' with np.nan for processing\n    engineered_data[column] = engineered_data[column].replace('Unknown', np.nan)\n    # Convert column to float to handle both integers and floating points\n    engineered_data[column] = engineered_data[column].astype(float)\n    # Calculate the median of the column, ignoring NaN values\n    median_value = engineered_data[column].median()\n    # Replace NaN values with the median value\n    engineered_data[column] = engineered_data[column].fillna(median_value)\n    # Convert the column to int64 by first rounding and then converting to int\n    engineered_data[column] = engineered_data[column].round().astype('int64')\n\n# Check the data types to confirm the conversion\nprint(engineered_data.dtypes)\ntrain_df = engineered_data","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:22:04.130637Z","iopub.execute_input":"2024-03-16T20:22:04.131104Z","iopub.status.idle":"2024-03-16T20:22:04.369742Z","shell.execute_reply.started":"2024-03-16T20:22:04.131063Z","shell.execute_reply":"2024-03-16T20:22:04.368602Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"RecordID             int64\nChampionshipYear     int64\nCustomerID           int64\nCustomerCity        object\nCustomerState       object\n                     ...  \nBuffalo              int64\nMercer               int64\nHoly cross           int64\nVermont              int64\nIsLocalEvent         int64\nLength: 147, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imputing it for the test set","metadata":{}},{"cell_type":"code","source":"\n# Assuming engineered_data is your DataFrame\n\n# List of columns to convert to int64\ncolumns_to_convert = ['Distance', 'Age', 'Begin_day']\n\nfor column in columns_to_convert:\n    # Replace 'Unknown' with np.nan for processing\n    engineered_data_test[column] = engineered_data_test[column].replace('Unknown', np.nan)\n    # Convert column to float to handle both integers and floating points\n    engineered_data_test[column] = engineered_data_test[column].astype(float)\n    # Calculate the median of the column, ignoring NaN values\n    median_value = engineered_data_test[column].median()\n    # Replace NaN values with the median value\n    engineered_data_test[column] = engineered_data_test[column].fillna(median_value)\n    # Convert the column to int64 by first rounding and then converting to int\n    engineered_data_test[column] = engineered_data_test[column].round().astype('int64')\n\n# Check the data types to confirm the conversion\nprint(engineered_data_test.dtypes)\ntest_df = engineered_data_test","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:22:04.371419Z","iopub.execute_input":"2024-03-16T20:22:04.372568Z","iopub.status.idle":"2024-03-16T20:22:04.413787Z","shell.execute_reply.started":"2024-03-16T20:22:04.372521Z","shell.execute_reply":"2024-03-16T20:22:04.412207Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"RecordID             int64\nChampionshipYear     int64\nCustomerID           int64\nCustomerCity        object\nCustomerState       object\n                     ...  \nIUPUI                int64\nMarquette            int64\nGardner-Webb         int64\nJames madison        int64\nIsLocalEvent         int64\nLength: 147, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Checking for datatype inconsistency","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Sample DataFrame creation for demonstration; replace it with your actual DataFrame\n# train_df = pd.read_csv('your_file.csv')\n\ndef find_mixed_type_columns(df):\n    mixed_type_columns = {}\n    for column in df.columns:\n        # Using a set to store unique types found in each column\n        unique_types = {type(value).__name__ for value in df[column] if pd.notnull(value)}\n        if len(unique_types) > 1:\n            mixed_type_columns[column] = unique_types\n    return mixed_type_columns\n\nmixed_type_columns = find_mixed_type_columns(train_df)\nfor column, types in mixed_type_columns.items():\n    print(f\"Column '{column}' contains these types: {types}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:22:04.415525Z","iopub.execute_input":"2024-03-16T20:22:04.415999Z","iopub.status.idle":"2024-03-16T20:22:52.370570Z","shell.execute_reply.started":"2024-03-16T20:22:04.415959Z","shell.execute_reply":"2024-03-16T20:22:52.369079Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# No inconsistency found!","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:22:52.372367Z","iopub.execute_input":"2024-03-16T20:22:52.373197Z","iopub.status.idle":"2024-03-16T20:22:52.378328Z","shell.execute_reply.started":"2024-03-16T20:22:52.373153Z","shell.execute_reply":"2024-03-16T20:22:52.377006Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Now preparing to impute medians for other numerical data","metadata":{}},{"cell_type":"code","source":"\ntrain_df.replace('Unknown', np.nan, inplace=True)\ntest_df.replace('Unknown', np.nan, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:22:52.382283Z","iopub.execute_input":"2024-03-16T20:22:52.383139Z","iopub.status.idle":"2024-03-16T20:22:54.096177Z","shell.execute_reply.started":"2024-03-16T20:22:52.383093Z","shell.execute_reply":"2024-03-16T20:22:54.094469Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Load datasets\n#train_df = pd.read_csv('DIWBB_Training.csv')\n#test_df = pd.read_csv('DIWBB_Test.csv')\n\ntrain_df.columns = train_df.columns.astype(str)\ntest_df.columns = test_df.columns.astype(str)\n\n# Identify columns by type\ncategorical_cols = train_df.select_dtypes(include=['object', 'bool','string']).columns.tolist()\nnumerical_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Remove target variable and identifiers from feature list\ncategorical_cols.remove('ActivityType')\nnumerical_cols.remove('RecordID')\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Preprocess the datasets\nX_train = preprocessor.fit_transform(train_df.drop(['RecordID', 'ActivityType'], axis=1))\nX_test = preprocessor.transform(test_df.drop('RecordID', axis=1))\n\n# Target variable\ny_train = train_df['ActivityType']\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:22:54.098245Z","iopub.execute_input":"2024-03-16T20:22:54.100239Z","iopub.status.idle":"2024-03-16T20:23:08.365727Z","shell.execute_reply.started":"2024-03-16T20:22:54.100172Z","shell.execute_reply":"2024-03-16T20:23:08.364193Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## We can see the dimensions of the sparse matrix below","metadata":{}},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:23:08.367566Z","iopub.execute_input":"2024-03-16T20:23:08.368029Z","iopub.status.idle":"2024-03-16T20:23:08.375797Z","shell.execute_reply.started":"2024-03-16T20:23:08.367983Z","shell.execute_reply":"2024-03-16T20:23:08.374731Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<209266x33354 sparse matrix of type '<class 'numpy.float64'>'\n\twith 30343570 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:23:08.377582Z","iopub.execute_input":"2024-03-16T20:23:08.378372Z","iopub.status.idle":"2024-03-16T20:23:08.390913Z","shell.execute_reply.started":"2024-03-16T20:23:08.378331Z","shell.execute_reply":"2024-03-16T20:23:08.389501Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<20935x33354 sparse matrix of type '<class 'numpy.float64'>'\n\twith 3026526 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Code to see sparse matrix in detail","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.sparse import csr_matrix\n\n# Assuming X_train is your sparse matrix\n# Convert the sparse matrix to a dense matrix first\ndense_matrix = X_train.toarray()\n\n# Generate generic column names\ncolumns = [f'feature_{i}' for i in range(X_train.shape[1])]\n\n# Create a DataFrame\ndf_xpanded = pd.DataFrame(data=dense_matrix, columns=columns)\n\n# Now df is a full DataFrame of your sparse matrix\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:23:08.392610Z","iopub.execute_input":"2024-03-16T20:23:08.393122Z","iopub.status.idle":"2024-03-16T20:23:24.867829Z","shell.execute_reply.started":"2024-03-16T20:23:08.393084Z","shell.execute_reply":"2024-03-16T20:23:24.866461Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df_xpanded","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:23:24.869793Z","iopub.execute_input":"2024-03-16T20:23:24.870714Z","iopub.status.idle":"2024-03-16T20:23:25.708348Z","shell.execute_reply.started":"2024-03-16T20:23:24.870669Z","shell.execute_reply":"2024-03-16T20:23:25.706889Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"        feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n0       -1.059492  -1.725058   0.114379  -0.889407   -0.10453  -0.257176   \n1        0.943848  -1.725058   0.114379  -0.611291   -0.10453  -0.257176   \n2       -1.059492  -1.725023   0.114379   0.501175   -0.10453  -0.257176   \n3        0.943848  -1.725023   0.114379   0.779291   -0.10453  -0.257176   \n4       -1.059492  -1.724934   0.114379   0.501175   -0.10453  -0.257176   \n...           ...        ...        ...        ...        ...        ...   \n209261   0.943848   1.729360   0.114379  -2.001873   -0.10453  -0.257176   \n209262  -1.059492   1.729375   0.114379   0.501175   -0.10453  -0.257176   \n209263   0.943848   1.729375   0.114379   0.779291   -0.10453  -0.257176   \n209264  -1.059492   1.729399   0.114379   0.501175   -0.10453  -0.257176   \n209265   0.943848   1.729399   0.114379   0.779291   -0.10453  -0.257176   \n\n        feature_6  feature_7  feature_8  feature_9  ...  feature_33344  \\\n0       -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n1       -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n2       -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n3       -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n4       -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n...           ...        ...        ...        ...  ...            ...   \n209261   0.770395   2.146582  -0.052215  -0.155377  ...            0.0   \n209262  -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n209263  -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n209264  -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n209265  -0.330742  -0.444656  -0.052215  -0.155377  ...            0.0   \n\n        feature_33345  feature_33346  feature_33347  feature_33348  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n209261            0.0            0.0            0.0            0.0   \n209262            0.0            0.0            0.0            0.0   \n209263            0.0            0.0            0.0            0.0   \n209264            0.0            0.0            0.0            0.0   \n209265            0.0            0.0            0.0            0.0   \n\n        feature_33349  feature_33350  feature_33351  feature_33352  \\\n0                 0.0            1.0            0.0            0.0   \n1                 0.0            1.0            0.0            0.0   \n2                 0.0            1.0            0.0            0.0   \n3                 0.0            1.0            0.0            0.0   \n4                 0.0            1.0            0.0            0.0   \n...               ...            ...            ...            ...   \n209261            0.0            1.0            0.0            0.0   \n209262            0.0            1.0            0.0            0.0   \n209263            0.0            1.0            0.0            0.0   \n209264            0.0            1.0            0.0            0.0   \n209265            0.0            1.0            0.0            0.0   \n\n        feature_33353  \n0                 1.0  \n1                 1.0  \n2                 1.0  \n3                 1.0  \n4                 1.0  \n...               ...  \n209261            1.0  \n209262            1.0  \n209263            1.0  \n209264            1.0  \n209265            1.0  \n\n[209266 rows x 33354 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>feature_9</th>\n      <th>...</th>\n      <th>feature_33344</th>\n      <th>feature_33345</th>\n      <th>feature_33346</th>\n      <th>feature_33347</th>\n      <th>feature_33348</th>\n      <th>feature_33349</th>\n      <th>feature_33350</th>\n      <th>feature_33351</th>\n      <th>feature_33352</th>\n      <th>feature_33353</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.059492</td>\n      <td>-1.725058</td>\n      <td>0.114379</td>\n      <td>-0.889407</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.943848</td>\n      <td>-1.725058</td>\n      <td>0.114379</td>\n      <td>-0.611291</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.059492</td>\n      <td>-1.725023</td>\n      <td>0.114379</td>\n      <td>0.501175</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.943848</td>\n      <td>-1.725023</td>\n      <td>0.114379</td>\n      <td>0.779291</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.059492</td>\n      <td>-1.724934</td>\n      <td>0.114379</td>\n      <td>0.501175</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>209261</th>\n      <td>0.943848</td>\n      <td>1.729360</td>\n      <td>0.114379</td>\n      <td>-2.001873</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>0.770395</td>\n      <td>2.146582</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>209262</th>\n      <td>-1.059492</td>\n      <td>1.729375</td>\n      <td>0.114379</td>\n      <td>0.501175</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>209263</th>\n      <td>0.943848</td>\n      <td>1.729375</td>\n      <td>0.114379</td>\n      <td>0.779291</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>209264</th>\n      <td>-1.059492</td>\n      <td>1.729399</td>\n      <td>0.114379</td>\n      <td>0.501175</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>209265</th>\n      <td>0.943848</td>\n      <td>1.729399</td>\n      <td>0.114379</td>\n      <td>0.779291</td>\n      <td>-0.10453</td>\n      <td>-0.257176</td>\n      <td>-0.330742</td>\n      <td>-0.444656</td>\n      <td>-0.052215</td>\n      <td>-0.155377</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>209266 rows × 33354 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install xgboost","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:23:25.710305Z","iopub.execute_input":"2024-03-16T20:23:25.711523Z","iopub.status.idle":"2024-03-16T20:23:44.036552Z","shell.execute_reply.started":"2024-03-16T20:23:25.711456Z","shell.execute_reply":"2024-03-16T20:23:44.034672Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.11.4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Importing the libraries. (Skipped splitting as we are training on all the data)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:23:44.039331Z","iopub.execute_input":"2024-03-16T20:23:44.039906Z","iopub.status.idle":"2024-03-16T20:23:44.171515Z","shell.execute_reply.started":"2024-03-16T20:23:44.039825Z","shell.execute_reply":"2024-03-16T20:23:44.169923Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Building our ensemble model\n","metadata":{"execution":{"iopub.execute_input":"2024-02-26T20:14:36.330605Z","iopub.status.busy":"2024-02-26T20:14:36.330201Z","iopub.status.idle":"2024-02-26T20:14:36.335349Z","shell.execute_reply":"2024-02-26T20:14:36.334250Z","shell.execute_reply.started":"2024-02-26T20:14:36.330576Z"}}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import accuracy_score\nimport joblib  # Import joblib for saving the model\n\n# Assuming X_train, y_train are your full training dataset\n\n# Encode the target variable if necessary\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\n\n\n# Initialize models\nlogistic_model = LogisticRegression(max_iter=10000, random_state=0, C=0.5)\nlinear_svc = LinearSVC(random_state=96, C=1.0, tol=1e-3, max_iter=15000, dual=False)\nxgb_model = XGBClassifier(n_estimators=3000, learning_rate=0.1, random_state=69)\n\n# Calling the callibrated_svc so that model can fit into ensemble.\ncalibrated_svc = CalibratedClassifierCV(linear_svc, method='sigmoid', cv=5)\n\n# Ensemble model with VotingClassifier\nensemble_model = VotingClassifier(\n    estimators=[\n        ('logistic', logistic_model),\n        ('svc', calibrated_svc),\n        ('xgb', xgb_model)\n    ],\n    voting='soft'\n)\n\n# Train the ensemble model on the full training dataset\nensemble_model.fit(X_train, y_train_encoded)\n \n \n","metadata":{"execution":{"iopub.execute_input":"2024-02-29T02:00:29.168745Z","iopub.status.busy":"2024-02-29T02:00:29.168325Z","iopub.status.idle":"2024-02-29T02:10:13.830739Z","shell.execute_reply":"2024-02-29T02:10:13.829880Z","shell.execute_reply.started":"2024-02-29T02:00:29.168715Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now if we want to see the cross validation of the above ensemble model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import accuracy_score\nimport joblib  # Import joblib for saving the model\n\nfrom sklearn.model_selection import cross_val_score\n\nlabel_encoder = LabelEncoder()\n# Assuming X and y are your full dataset and target variable respectively\n# Ensure y is encoded if it's categorical\ny_encoded = label_encoder.fit_transform(y_train)\n\n# Initialize the models again as needed\nlogistic_model = LogisticRegression(max_iter=10000, random_state=0, C=0.5)\nlinear_svc = LinearSVC(random_state=96, C=1.0, tol=1e-3, max_iter=15000, dual=False)\ncalibrated_svc = CalibratedClassifierCV(estimator=linear_svc, method='sigmoid', cv=5)\nxgb_model = XGBClassifier(n_estimators=3000, learning_rate=0.1, random_state=69)\n\n# Initialize the ensemble model with the same configurations\nensemble_model = VotingClassifier(\n    estimators=[\n        ('logistic', logistic_model),\n        ('svc', calibrated_svc),\n        ('xgb', xgb_model)\n    ],\n    voting='soft'\n)\n\n# Perform 5-fold cross-validation\n# Adjust cv parameter as needed for your application\ncv_scores = cross_val_score(ensemble_model, X_train, y_encoded, cv=5, scoring='accuracy')\n\n# Print the cross-validation scores\nprint(\"CV Scores:\", cv_scores)\n\n# Print the average of the cross-validation scores\nprint(\"Average CV Score:\", cv_scores.mean())\n","metadata":{"execution":{"iopub.execute_input":"2024-02-29T22:03:32.643343Z","iopub.status.busy":"2024-02-29T22:03:32.643063Z","iopub.status.idle":"2024-02-29T22:26:47.785240Z","shell.execute_reply":"2024-02-29T22:26:47.784509Z","shell.execute_reply.started":"2024-02-29T22:03:32.643287Z"}},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"CV Scores: [0.99084914 0.99137457 0.99108786 0.99139847 0.99130289]\n\nAverage CV Score: 0.9912025859597055\n"}]},{"cell_type":"markdown","source":"# Now seeing the feature importances","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import accuracy_score\nimport joblib  # Import joblib for saving the model\n\n\n\nlabel_encoder = LabelEncoder()\n# Assuming X and y are your full dataset and target variable respectively\n# Ensure y is encoded if it's categorical\ny_encoded = label_encoder.fit_transform(y_train)\n\n# Directly fit an XGBoost model\nxgb_model = XGBClassifier(n_estimators=3000, learning_rate=0.1, random_state=69)\nxgb_model.fit(X_train, y_encoded)\n\n\n# Get feature importances\nfeature_importances = xgb_model.feature_importances_\n\n# Assuming you have a way to accurately retrieve or define feature names\n# This step might require custom handling based on how your pipeline is set up\nfeature_names = np.array([f\"Feature {i}\" for i in range(X_train.shape[1])])  # Placeholder feature names\n\n# Combine feature names with their importances and sort them\nfeatures_and_importances = sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)\n\n# Display the top 10 feature importances\nprint(\"Top 10 Feature Importances from XGBoost:\")\nfor feature, importance in features_and_importances[:10]:\n    print(f\"{feature}: {importance}\")\n","metadata":{"execution":{"iopub.execute_input":"2024-03-05T23:05:42.801275Z","iopub.status.busy":"2024-03-05T23:05:42.801041Z","iopub.status.idle":"2024-03-05T23:10:03.802112Z","shell.execute_reply":"2024-03-05T23:10:03.801124Z","shell.execute_reply.started":"2024-03-05T23:05:42.801252Z"}},"execution_count":48,"outputs":[{"name":"stdout","output_type":"stream","text":"Top 10 Feature Importances from XGBoost:\n\nFeature 24271: 0.40902844071388245\n\nFeature 24272: 0.2712523937225342\n\nFeature 24387: 0.07418902963399887\n\nFeature 24275: 0.030830474570393562\n\nFeature 24267: 0.008823391050100327\n\nFeature 24270: 0.004186241887509823\n\nFeature 24543: 0.0037762883584946394\n\nFeature 0: 0.003322502365335822\n\nFeature 24269: 0.002695696661248803\n\nFeature 24471: 0.0025133206509053707\n"}]},{"cell_type":"code","source":"# Assuming preprocessor is your ColumnTransformer\n\n# Use get_feature_names_out to retrieve the transformed feature names\n# For versions of scikit-learn prior to 1.0, you might need to use get_feature_names() method with adjustments\nfeature_names = preprocessor.get_feature_names_out()\n\n# N\n\n# Get feature importances\nfeature_importances = xgb_model.feature_importances_\n\n# Combine feature names with their importances and sort them\nfeatures_and_importances = sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)\n\n# Display the top feature importances\nprint(\"Top Feature Importances from XGBoost:\")\nfor feature, importance in features_and_importances[:10]:\n    print(f\"{feature}: {importance}\")\n","metadata":{"execution":{"iopub.execute_input":"2024-03-05T23:13:28.969704Z","iopub.status.busy":"2024-03-05T23:13:28.969209Z","iopub.status.idle":"2024-03-05T23:13:29.078874Z","shell.execute_reply":"2024-03-05T23:13:29.078018Z","shell.execute_reply.started":"2024-03-05T23:13:28.969665Z"}},"execution_count":50,"outputs":[{"name":"stdout","output_type":"stream","text":"Top Feature Importances from XGBoost:\n\ncat__EventRoundName_missing: 0.40902844071388245\n\ncat__IsEventFinalSite_No: 0.2712523937225342\n\ncat__FacilityDescription_Professional Sports Arena: 0.07418902963399887\n\ncat__EventSession_All-Session: 0.030830474570393562\n\ncat__EventRoundName_Finals: 0.008823391050100327\n\ncat__EventRoundName_Regionals: 0.004186241887509823\n\ncat__Check_No: 0.0037762883584946394\n\nnum__ChampionshipYear: 0.003322502365335822\n\ncat__EventRoundName_First and Second Rounds: 0.002695696661248803\n\ncat__FacilityZipCode_missing: 0.0025133206509053707\n"}]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Assuming X and y_encoded are your features and encoded target variable respectively\n\n# Initialize your models\nlogistic_model = LogisticRegression(max_iter=10000, random_state=0, C=0.5)\nlinear_svc = LinearSVC(random_state=96, C=1.0, tol=1e-3, max_iter=15000, dual=False)\ncalibrated_svc = CalibratedClassifierCV(estimator=linear_svc, method='sigmoid', cv=5)\nxgb_model = XGBClassifier(n_estimators=3000, learning_rate=0.1, random_state=69)\n\n# Calculate cross-validated accuracies\ncv_score_logistic = cross_val_score(logistic_model, X_train, y_encoded, cv=5, scoring='accuracy').mean()\ncv_score_svc = cross_val_score(calibrated_svc, X_train, y_encoded, cv=5, scoring='accuracy').mean()\ncv_score_xgb = cross_val_score(xgb_model, X_train, y_encoded, cv=5, scoring='accuracy').mean()\n\nprint(\"Cross-validated Accuracy Scores:\")\nprint(f\"Logistic Regression: {cv_score_logistic:.4f}\")\nprint(f\"Calibrated Linear SVC: {cv_score_svc:.4f}\")\nprint(f\"XGBoost Classifier: {cv_score_xgb:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test data. Remember test data has been transformed before","metadata":{}},{"cell_type":"code","source":"\ndf = X_test\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the models\n\n\n# Make predictions\nensemble_predictions = ensemble_model.predict(df)\n\n","metadata":{"execution":{"iopub.execute_input":"2024-02-29T02:10:33.126278Z","iopub.status.busy":"2024-02-29T02:10:33.125885Z","iopub.status.idle":"2024-02-29T02:10:33.742057Z","shell.execute_reply":"2024-02-29T02:10:33.740838Z","shell.execute_reply.started":"2024-02-29T02:10:33.126245Z"}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Counting predictions","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n\n# Count the occurrences of each unique value in the array\nvalue_counts = Counter(ensemble_predictions)\n\n# Display the counts\nprint(value_counts)","metadata":{"execution":{"iopub.execute_input":"2024-02-29T02:10:33.744589Z","iopub.status.busy":"2024-02-29T02:10:33.744201Z","iopub.status.idle":"2024-02-29T02:10:33.755347Z","shell.execute_reply":"2024-02-29T02:10:33.754466Z","shell.execute_reply.started":"2024-02-29T02:10:33.744551Z"}},"execution_count":27,"outputs":[{"name":"stdout","output_type":"stream","text":"Counter({1: 19118, 3: 1356, 5: 380, 4: 57, 0: 20, 2: 4})\n"}]},{"cell_type":"code","source":"ensemble_predictions_labels = label_encoder.inverse_transform(ensemble_predictions)\n\n# Now, count the occurrences of each unique value in the array of labels\nvalue_counts_labels = Counter(ensemble_predictions_labels)\n\n# Display the counts with categories\nprint(value_counts_labels)","metadata":{"execution":{"iopub.execute_input":"2024-02-29T02:10:34.163547Z","iopub.status.busy":"2024-02-29T02:10:34.162877Z","iopub.status.idle":"2024-02-29T02:10:34.170444Z","shell.execute_reply":"2024-02-29T02:10:34.169539Z","shell.execute_reply.started":"2024-02-29T02:10:34.163511Z"}},"execution_count":28,"outputs":[{"name":"stdout","output_type":"stream","text":"Counter({'No Activity': 19118, 'Primary Purchase': 1356, 'Transfer Recipient': 380, 'Secondary Purchase': 57, 'Multiple Activities': 20, 'Other Secondary Activity': 4})\n"}]},{"cell_type":"code","source":"ensemble_predictions_labels = label_encoder.inverse_transform(ensemble_predictions)\n\n# Now, count the occurrences of each unique value in the array of labels\nvalue_counts_labels = Counter(ensemble_predictions_labels)\n\n# Display the counts with categories\nprint(value_counts_labels)","metadata":{"execution":{"iopub.execute_input":"2024-02-29T02:10:40.425790Z","iopub.status.busy":"2024-02-29T02:10:40.425395Z","iopub.status.idle":"2024-02-29T02:10:40.433200Z","shell.execute_reply":"2024-02-29T02:10:40.432381Z","shell.execute_reply.started":"2024-02-29T02:10:40.425755Z"}},"execution_count":29,"outputs":[{"name":"stdout","output_type":"stream","text":"Counter({'No Activity': 19118, 'Primary Purchase': 1356, 'Transfer Recipient': 380, 'Secondary Purchase': 57, 'Multiple Activities': 20, 'Other Secondary Activity': 4})\n"}]},{"cell_type":"markdown","source":"# Exporting","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming logistic_predictions is your array\n\n# Convert the array to a pandas DataFrame\ndf = pd.DataFrame(ensemble_predictions_labels, columns=['ActivityType'])\n\n# Export the DataFrame to a CSV file\ndf.to_csv('lastdata_28th_ensemble_droppedmulticollinearcolumns.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2024-02-29T02:11:31.303349Z","iopub.status.busy":"2024-02-29T02:11:31.302948Z","iopub.status.idle":"2024-02-29T02:11:31.323502Z","shell.execute_reply":"2024-02-29T02:11:31.322587Z","shell.execute_reply.started":"2024-02-29T02:11:31.303315Z"}},"execution_count":30,"outputs":[]}]}